conv1 (3 фильтра размера (5, 5)) 
→ maxpool1(ядро 2) 
→ conv2 (5 фильтров размера (3, 3))
→ maxpool2(ядро 2) 
→ flatten 
→ fc1(100 нейронов) 
→ fc2(10 нейронов)
class ConvNet(nn.Module):
    def __init__(self):
        super().__init__()

        # сверточные слои
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(5,5))
        self.pool1 = nn.MaxPool2d(kernel_size=(2,2))
        self.conv2 = nn.Conv2d(in_channels=3, out_channels=5, kernel_size=(3,3))
        self.pool2 = nn.MaxPool2d(kernel_size=(2,2))

        # вычисляем размерность после сверток и пулинга для fc слоя
        # вход 32x32 -> conv1(5x5): 28x28 -> pool1(2x2): 14x14 -> conv2(3x3): 12x12 -> pool2(2x2): 6x6
        self.flatten = nn.Flatten()
        
        # полносвязные слои
        self.fc1 = nn.Linear(5 * 6 * 6, 100)
        self.fc2 = nn.Linear(100, 10)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool1(x)
        x = F.relu(self.conv2(x))
        x = self.pool2(x)

        x = self.flatten(x)

        x = F.relu(self.fc1(x))
        x = self.fc2(x)

        return x

img = torch.Tensor(np.random.random((32, 3, 32, 32)))
model = ConvNet()
out = model(img)
print("Output shape:", out.shape)  # [32, 10]

class ConvNet(nn.Module):
    def __init__(self):
        super().__init__()

        # сверточные слои с большим числом фильтров + BatchNorm
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        
        self.pool = nn.MaxPool2d(2, 2)
        self.flatten = nn.Flatten()
        
        # полносвязные слои (добавим Dropout для регуляризации)
        self.fc1 = nn.Linear(64 * 8 * 8, 256)
        self.drop1 = nn.Dropout(0.5)
        self.fc2 = nn.Linear(256, 64)
        self.drop2 = nn.Dropout(0.3)
        self.fc3 = nn.Linear(64, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.bn1(self.conv1(x)))) # 32x16x16
        x = self.pool(F.relu(self.bn2(self.conv2(x)))) # 64x8x8

        x = self.flatten(x)

        x = F.relu(self.fc1(x))
        x = self.drop1(x)
        x = F.relu(self.fc2(x))
        x = self.drop2(x)
        x = self.fc3(x)

        return x

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = ConvNet().to(device)

loss_fn = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

model = train(model, loss_fn, optimizer, n_epoch=10)

test_accuracy, _ = evaluate(model, test_loader, loss_fn)
print('Accuracy на тесте:', test_accuracy)

if test_accuracy <= 0.5:
    print("Качество на тесте ниже 0.5, 0 баллов")
elif test_accuracy < 0.6:
    print("Качество на тесте между 0.5 и 0.6, 0.5 баллов")
elif test_accuracy >= 0.6:
    print("Качество на тесте выше 0.6, 1 балл")

model.eval()
x = torch.randn((1, 3, 32, 32))
torch.jit.save(torch.jit.trace(model.cpu(), (x)), "model.pth")
